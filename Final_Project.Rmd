---
title: "Final Project"
author: "Changhee Yoon"
date: "2022-11-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tsdl)
library(astsa)
library(tidyverse)
library(TSstudio)
library(fpp2)
library(MASS)
library(ggplot2)
library(ggfortify)
library(forecast)

# train <- read.csv("Data/train.csv")
train <- read.csv("Data/Beer.csv")
```

Dataset source: https://www.kaggle.com/datasets/sergiomora823/monthly-beer-production?select=datasets_56102_107707_monthly-beer-production-in-austr.csv

```{r}
df <- data.frame(date = train$Month, production = train$Monthly.beer.production)
df_ts <- ts(df$production, frequency = 12, start = c(1956,1))
```
```{r}
length(df_ts)
```


Get the monthly beer production data from 1956 to 1978. Split the data to use the last two years of data for testing later.
```{r}
# split the data
train <- df[c(228:240),]
test <- df[c(241:204),]

train_ts <- ts(train$production, frequency = 12, start = c(1956,1))
test_ts <- ts(test$production, frequency = 12, start = c(1956,181))
```

```{r}
ts.plot(train_ts)
```
```{r}
ts.plot(test_ts)
```

```{r}
hist(train$production)
```
```{r}
acf(train$production, lag.max=100)
```
Histogram apears to be skewed and the ACF is out of the 95% interval from lag 0 to 12.

Run BoxCox transformation to find the optimal lambda
```{r}
trainTrans <- boxcox(train$production~ as.numeric(1:length(train$production)))
lambda <- trainTrans$x[which(trainTrans$y == max(trainTrans$y))]
lambda
```
The lambda calculated from BoxCox is very close to zero, so I choose log transformation.
```{r}
train_log <- train_ts
train_log <- log(train_log)
train_log
```
```{r}
plot.ts(train_log)
```
The variance appear to be more stable after the transform
```{r}
hist(train_log)
```
It seems closer to normal. The data seem more clustered around the center, and the histogram doesn't look skewed anymore.

```{r}
ggAcf(train_log)
```
Decomposition

```{r}
plot(decompose(train_log))
```
Differencing of (Ut)^.303
```{r}
var(train_log)
```
remove seasonality by differencing at lag=12
```{r}
train_log_12 <- diff(train_log, lag=12)
var(train_log_12)
```
```{r}
fit <- lm(train_log_12 ~ as.numeric(1:length(train_log_12)))

plot.ts(train_log_12)

abline(fit, col="red")
abline(h=mean(train_log_12), col="blue")
```
Seasonality no longer apparent and variance is lower.
```{r}
train_log_12_1 <- diff(train_log_12, lag=1)
var(train_log_12_1)
```
```{r}
fit <- lm(train_log_12_1 ~ as.numeric(1:length(train_log_12_1)))

plot.ts(train_log_12_1)

abline(fit, col="red")
abline(h=mean(train_log_12_1), col="blue")

```
No seasonarity, no trend after differentiating at lag 12 and then at lag 1. However, the variance is larger

```{r}
hist(train_log, density=20,breaks=20, col="blue", xlab="", prob=TRUE, main = "Histogram of log(U_t)")
m<-mean(train_log)
std<- sqrt(var(train_log))
curve( dnorm(x,m,std), add=TRUE )
```

```{r}
hist(train_log_12, density=20,breaks=20, col="blue", xlab="", prob=TRUE, main = "Histogram of log(U_t) differenced at lag 12")
m<-mean(train_log_12)
std<- sqrt(var(train_log_12))
curve( dnorm(x,m,std), add=TRUE )
```
The histogram of transformed data appears to be more normal with the data distributed more towards the center compared to before transformation

Now, looking at the ACF
```{r}
acf(train_log, lag.max=100, main="ACF of log(U_t)")
```
```{r}
acf(train_log_12, lag.max=80, main="ACF of log(U_t) differenced at lag 12")
pacf(train_log_12, lag.max=80, main="PACF of the log(U_t), differenced at lags 12")
```
We already have a quite nice looking ACF plot for differenced at lag 12

SARIMA (0 or 11,0,0 or 9)(4, 1, 1 or 5)$_{s=12}$

```{r}
acf(train_log_12_1, lag.max=80, main="ACF of log(U_t) differenced at lag 12 and 1")
pacf(train_log_12_1, lag.max=80, main="PACF of the log(U_t), differenced at lags 12 and 1")
```
Determining parameters for SARIMA for $log(U_t)$: 
- We applied one seasonal differencing so D=1 at lag s=12
- We applied one differencing to remove the trend, so d=1
- ACF shows a strong peak at h = 1s, and a smaller peak at h = 3s, 5s so Q=1, 3 or 5
- ACF seems to cut off after h = 1 or 3, so q=1 or 3
- PACF appears to have no particular peak at any seasonal lags, so P=0
- PACF shows strong peak at h=1 and 2 and smaller peaks that cuts off after h=5, so p=2 or p=5

SARIMA (p=2 or 5, d=1, q=1 or 4)x(P=0, D=1, Q=3)$_{s=12}$

Here are candidate models:
i. SARIMA (p=2,d=1,q=1)x(P=0,D=1,Q=3)$_{s=12}$
ii. SARIMA (p=2,d=1,q=4)x(P=0,D=1,Q=3)$_{s=12}$
iii. SARIMA (p=5,d=1,q=1)x(P=0,D=1,Q=3)$_{s=12}$
iv. SARIMA (p=5,d=1,q=4)x(P=0,D=1,Q=3)$_{s=12}$



```{r}
# model i
arima(train_log, order=c(2,1,1), seasonal = list(order = c(0,1,3), period = 12), method="ML")
```
```{r}
# model ii
arima(train_log, order=c(2,1,4), seasonal = list(order = c(0,1,3), period = 12), method="ML")
```
```{r}
# model iii
arima(train_log, order=c(5,1,1), seasonal = list(order = c(0,1,3), period = 12), method="ML")
```
```{r}
# model iv
arima(train_log, order=c(5,1,4), seasonal = list(order = c(0,1,3), period = 12), method="ML")
```
I choose model ii and iv because model ii and iv have the lowest AIC values.

SARIMA (p=2,d=1,q=4)x(P=0,D=1,Q=3)$_{s=12}$
SARIMA (p=5,d=1,q=4)x(P=0,D=1,Q=3)$_{s=12}$
 
Before the diagonstic checking, check for stationarity and invertibility
```{r}
fit.iii <- sarima(xdata = train_log,
  p = 5, d = 1, q = 4,
  P = 0, D = 1, Q = 3, S = 12,
  details = F)

fit.iii$fit$coef
```

```{r}
fit <- lm(fit.iii$fit$residuals ~ as.numeric(1:length(fit.iii$fit$residuals)))

plot.ts(fit.iii$fit$residuals)

abline(fit, col="red")
abline(h=mean(fit.iii$fit$residuals), col="blue")

```
```{r}
hist(fit.iii$fit$residuals, breaks = 20)
```
```{r}
qqnorm(fit.iii$fit$residuals)
qqline(fit.iii$fit$residuals)
```
Histogram and qqPlot look good in terms of normal distribution of residuals
```{r}
acf(fit.iii$fit$residuals)
```
```{r}
pacf(fit.iii$fit$residuals)
```
ACF and PACF plots look great

```{r}
shapiro.test(fit.iii$fit$residuals)
```
Shapiro Wilk normality test has p-value higher than .05, so fail to reject the null that residuals are normally distributed. It passes the normality test.

More diagonostic tests:

Box-Pierce test
```{r}
res <- fit.iii$fit$residuals
sqrt(length(res)) # h is about 15 which is sqrt(n)
```
h-(p+q) = 12 - 5 = 1
```{r}
Box.test(res, lag=13, type = c("Box-Pierce"), fitdf = 9)
```
Box-Ljung test
```{r}
Box.test(res, lag=12, type = c("Ljung-Box"), fitdf = 7)
```
McLeod-Li test
```{r}
Box.test(res^2, lag=12, type = c("Ljung-Box"), fitdf = 0)
```
We have all the p-values larger than .05, so the model passes the diagnostic tests

Since the model has passed the diagnostic checkings, it is the final model.
SARIMA (p=11,d=1,q=3)x(P=0,D=1,Q=3)$_{s=12}$

SARIMA (p=2,d=1,q=3)x(P=0,D=1,Q=1)$_{s=12}$
### Forcast
```{r}
test_log <- log(test_ts)
```

```{r}
sarima.for(train_log, n.ahead=24 ,p=2, d=1, q=3, P=0, D=1, Q=1, S=12)
lines(test_log, col="blue", type="b")
```

