---
title: "Australian Beer Production Forecast"
author: "Changhee (Eric) Yoon"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tsdl)
library(astsa)
library(tidyverse)
library(TSstudio)
library(fpp2)
library(MASS)
library(ggplot2)
library(ggfortify)
library(forecast)
library(nlme)
library(MuMIn)
library(UnitCircle)

# train <- read.csv("Data/train.csv")
train <- read.csv("Data/Beer.csv")
```

#Abstract

#Introduction

The goal of this project is to implement time series model with parameters tuned on 20 years worth data from 1956 to 1976 to make a 2 year forecast on the monthly beer production (from 1976~1978). The dataset consists of two columns: date and beer production in million liters. It is important to 

It is crucial for the businesses to predict how much production of the product is needed for each month/time of the year, and having a good, accurate model will provide forecasts that will be beneficial insights for businesses to prepare resources to produce the appropriate amount of product needed each month. Inaccurate forecast would result in over or under producing the goods and result in financial losses for the businesses. The Australian beer industry produced about 1.6 billion liters (about 422 million gallons) of beer in 2018, and the beer industry contributed 4.6 billion Australian dollars to the Australian economy. In an industry of this size, it's crucial to have an accurate forecast of the production.

In order to make an accurate forecast, I will be considering the seasonal component and the trend of the data when considering the seasonal parameter and the differencing parameter

Dataset source: https://www.kaggle.com/datasets/sergiomora823/monthly-beer-production?select=datasets_56102_107707_monthly-beer-production-in-austr.csv

```{r}
df <- data.frame(date = train$Month, production = train$Monthly.beer.production)
df_ts <- ts(df$production, frequency = 12, start = c(1956,1))
```

Get the monthly beer production data from 1956 to 1978. Split the data to use the last two years of data for testing later.
```{r}
# split the data
train <- df[c(1:240),]
test <- df[c(241:264),]

train_ts <- ts(train$production, frequency = 12, start = c(1956,1))
test_ts <- ts(test$production, frequency = 12, start = c(1956,241))
```

```{r}
ts.plot(train_ts, ylab="Beer Production - Million Liters")
```
The graph of time series shows that there is a positive trend over time. The data appears to have a seasonal component with yearly peaks and troughs. There aren't any apparent sharp changes in behavior.
```{r}
ts.plot(test_ts)
```

```{r}
hist(train$production)
```
```{r}
acf(train$production, lag.max=100)
```
Histogram appears to be skewed and the ACF is large and periodic
Variance appears to increase over time and needs to be stabilized

Run BoxCox transformation to find the optimal lambda
```{r}
trainTrans <- boxcox(train$production~ as.numeric(1:length(train$production)))
lambda <- trainTrans$x[which(trainTrans$y == max(trainTrans$y))]
lambda
```
The lambda calculated from BoxCox is very close to zero, so I choose log transformation.
```{r}
train_log <- train_ts
train_log <- log(train_log)
train_log
```
```{r}
plot.ts(train_log)
```
The variance appear to be more stable after the transform
```{r}
hist(train_log)
```
It seems closer to normal. The data seem more clustered around the center, and the histogram doesn't look skewed anymore.

```{r}
acf(train_log, lag.max=100)
```
Decomposition

```{r}
plot(decompose(train_log))
```
Differencing of (Ut)^.303
```{r}
var(train_log)
```
remove seasonality by differencing at lag=12
```{r}
train_log_12 <- diff(train_log, lag=12)
var(train_log_12)
```
```{r}
fit <- lm(train_log_12 ~ as.numeric(1:length(train_log_12)))

plot.ts(train_log_12)

abline(fit, col="red")
abline(h=mean(train_log_12), col="blue")
```
Seasonality no longer apparent and variance is lower.

SARIMA (0 or 11,0,0 or 9)(4, 1, 1 or 5)$_{s=12}$
```{r}
# Candidate models:
df <- expand.grid(p=0:11, q=0:9, P=4, Q=0:5)
df <- cbind(df, AICc=NA)
# Compute AICc:
for (i in 1:nrow(df)) {
  sarima.obj <- NULL
  try(arima.obj <- arima(train_log, order=c(df$p[i], 0, df$q[i]),
                         seasonal=list(order=c(df$P[i], 1, df$Q[i]), period=12),
                         method="ML"))
  if (!is.null(arima.obj)) { df$AICc[i] <- AICc(arima.obj) }
  # print(df[i, ])
}
df[which.min(df$AICc), ]
```


```{r}
# Differencing at lag 12 and 1
train_log_12_1 <- diff(train_log_12, lag=1)
var(train_log_12_1)
```
```{r}
fit <- lm(train_log_12_1 ~ as.numeric(1:length(train_log_12_1)))

plot.ts(train_log_12_1)

abline(fit, col="red")
abline(h=mean(train_log_12_1), col="blue")

```
No seasonarity, no trend after differentiating at lag 12 and then at lag 1. However, the variance is larger

```{r}
hist(train_log, density=20,breaks=20, col="blue", xlab="", prob=TRUE, main = "Histogram of log(U_t)")
m<-mean(train_log)
std<- sqrt(var(train_log))
curve( dnorm(x,m,std), add=TRUE )
```

```{r}
hist(train_log_12, density=20,breaks=20, col="blue", xlab="", prob=TRUE, main = "Histogram of log(U_t) differenced at lag 12")
m<-mean(train_log_12)
std<- sqrt(var(train_log_12))
curve( dnorm(x,m,std), add=TRUE )
```
The histogram of transformed data appears to be more normal with the data distributed more towards the center compared to before transformation

Now, looking at the ACF
```{r}
acf(train_log, lag.max=100, main="ACF of log(U_t)")
```
```{r}
acf(train_log_12, lag.max=80, main="ACF of log(U_t) differenced at lag 12")
pacf(train_log_12, lag.max=80, main="PACF of the log(U_t), differenced at lags 12")
```
SARIMA (p=0 or 1 or 3,d=0,q=0 or 3)(4, 1, 1 or 5)$_{s=12}$

We already have a quite nice looking ACF plot for differenced at lag 12

```{r}
acf(train_log_12_1, lag.max=80, main="ACF of log(U_t) differenced at lag 12 and 1")
pacf(train_log_12_1, lag.max=80, main="PACF of the log(U_t), differenced at lags 12 and 1")
```
Determining parameters for SARIMA for $log(U_t)$: 
- We applied one seasonal differencing so D=1 at lag s=12
- We applied one differencing to remove the trend, so d=1
- ACF seems significant (outside of 95% CI) at h=1s, 3s, and 5s, so Q=1 or 3 or 5
- ACF shows strong peak at h=1 and smaller peak at h=3 and maybe at h=4, so q=1 or 3 or 4
- PACF appears to have no particular peak at any seasonal lags, so P=0
- PACF shows strong peaks peaks at h = 1, 2 and smaller peaks until h=5 and there is a quite strong peak at h=11, so p=1 or 2 or 5

Here are candidate models:

For differencing at lag 12:

- SARIMA (0 or 11,0,0 or 9)(4, 1, 1 or 5)$_{s=12}$
```{r}
arima(train_log, order=c(0,0,0), seasonal = list(order = c(4,1,5), period = 12), method="ML")
```

For differencing at lag 12 and 1:

- SARIMA (p=2 or 5 or 11, d=1, q=1 or 3 or 4)x(P=0,D=1,Q=1 or 3 or 5)$_{s=12}$

After fitting models with all the possible combination of parameters, the model with d=0 all did not pass Shapiro Wilks normality test on the residuals. 

For the models with d=1 (differenced at lag 12 and 1), I've tried all the possible combinations of the parameters and here are two candidates based on the low AIC scores:

model i: SARIMA (p=2, d=1, q=3)x(P=0,D=1,Q=3)$_{s=12}$
```{r}
arima(train_log, order=c(2,1,3), seasonal = list(order = c(0,1,3), period = 12), method="ML")
```
```{r}
arima(train_log, order=c(2,1,3), seasonal = list(order = c(0,1,1), period = 12), method="ML")
```
model ii: SARIMA (p=11, d=1, q=3)x(P=0,D=1,Q=5)$_{s=12}$
```{r}
arima(train_log, order=c(11,1,3), seasonal = list(order = c(0,1,5), period = 12), method="ML")
```
$$\phi_1$$

###Model i
```{r}
fit.i <- sarima(xdata = train_log,
  p = 2, d = 1, q = 3,
  P = 0, D = 1, Q = 3, S = 12,
  details = F)


resi <- fit.i$fit$residuals

fit.i$fit$coef
```
In mathmatical equation:
$$\left(1-\phi _1B-\phi _2B^2\right)\left(1-B\right)\left(1-B^{12}\right)X_t=\left(1+\theta _1B+\theta _2B^2+\theta _3B^3\right)\left(1+\Theta_1B^{12}+\Theta_2B^{24}+\Theta_3B^{36}\right)Z_t$$

$$(1-B^{12}-B+B^{13}-\phi_1B+\phi_1B^{13}+\phi_1B^2-\phi_1B^{14}-B^2\phi_2+B^{14}\phi_2+B^3\phi_2-B^{15}\phi_2)X_t=(1+B^{12}\Theta_1+B^{24}\Theta_2+B^{36}\Theta_3+Bθ_1+θ_1B^{13}\Theta_1+θ_1B^{25}\Theta_2+θ_1B^{37}\Theta_3+B^2θ_2+B^{14}θ_2\Theta_1+B^{26}θ_2\Theta_2+B^{38}θ_2\Theta_3+B^3θ_3+B^{15}θ_3\Theta_1+B^{27}θ_3\Theta_2+B^{39}θ_3\Theta_3)Z_t$$
Before the diagnostic checking, check the stationarity and invertibility

___ 

SARIMA is confusing...

"Unit roots" - means the roots of polynomial is 1? or inside of the unit circle???

Unit roots in the MA part = over differencing
                  AR part = under differencing
  Do I check for unit roots with parameters of seasonality part in characteristic polynomial?

Roots outside of unit circle in the AR part - stationary
                                    MA part - invertible
___

For AR component:
```{r}
uc.check(pol_ = c(1, 1.152227, 0.999787), plot_output = TRUE)
```
For MA component:
```{r}
uc.check(pol_ = c(-0.83694557, -0.03719675, 0.09443777), plot_output = TRUE)
```


```{r}
fit <- lm(resi ~ as.numeric(1:length(resi)))

plot.ts(resi)

abline(fit, col="red")
abline(h=mean(resi), col="blue")

```
```{r}
hist(resi, breaks = 20)
```
```{r}
qqnorm(resi)
qqline(resi)
```
Histogram and qqPlot look good in terms of normal distribution of residuals
```{r}
acf(resi)
```
```{r}
pacf(resi)
```
ACF and PACF plots look great

```{r}
shapiro.test(resi)
```
Shapiro Wilk normality test has p-value higher than .05, so fail to reject the null that residuals are normally distributed. It passes the normality test.

More diagonostic tests:

Box-Pierce test
```{r}
sqrt(length(resi)) # h is about 15 which is sqrt(n)
```
h-(p+q) = 15 - 5 = 10
```{r}
Box.test(resi, lag=15, type = c("Box-Pierce"), fitdf = 5)
```
Box-Ljung test
```{r}
Box.test(resi, lag=15, type = c("Ljung-Box"), fitdf = 5)
```
McLeod-Li test
```{r}
Box.test(res, lag=15, type = c("Ljung-Box"), fitdf = 0)
```
Unfortunately, the model i does not pass Box-Ljung test since the p-value is smaller than .05

###Model ii
Model ii
```{r}
fit.ii <- sarima(xdata = train_log,
  p = 1, d = 1, q = 3,
  P = 0, D = 1, Q = 5, S = 12,
  details = F)

resii <- fit.ii$fit$residuals

fit.ii$fit$coef
```

In mathmatical equation:
$$(1+\phi _1B+\phi _2B^2+\phi \:_3B^3+\phi \:_4B^4+\phi \:_5B^5+\phi \:_6B^6+\phi \:_7B^7+\phi \:_8B^8+\phi \:_9B^9+\phi _{10}B^{10}+\phi \:_{11}B^{11})\left(1-B\right)\left(1-B^{12}\right)X_t=\left(1+\theta _1B+\theta _2B^2+\theta _3B^3\right)\left(1+\Theta _1B^{12}+\Theta _2B^{24}+\Theta _3B^{36}+\Theta \:_4B^{48}+\Theta \:_5B^{60}\right)Z_t$$
Check for
For AR component:
```{r}
uc.check(pol_ = c(1, 0.54486539), plot_output = TRUE)
```
For MA component:
```{r}
uc.check(pol_ = c(1, -0.43296268, -0.58201823,  0.14990599), plot_output = TRUE)
```

For seasonal MA component:
```{r}
uc.check(pol_ = c(1, -0.71879831, -0.16573458, -0.10873247,  0.09960884,  0.25077335 ), plot_output = TRUE)
```


```{r}
fit <- lm(resii ~ as.numeric(1:length(resii)))

plot.ts(resii)

abline(fit, col="red")
abline(h=mean(resii), col="blue")

```
```{r}
hist(resii, breaks = 20)
```
```{r}
qqnorm(resii)
qqline(resii)
```
Histogram and qqPlot look good in terms of normal distribution of residuals
```{r}
acf(resii)
```
```{r}
pacf(resii)
```
ACF and PACF plots look great

```{r}
shapiro.test(resii)
```
Shapiro Wilk normality test has p-value higher than .05, so fail to reject the null that residuals are normally distributed. It passes the normality test.

More diagonostic tests:

Box-Pierce test
```{r}
sqrt(length(resii)) # h is about 15 which is sqrt(n)
```
h-(p+q) = 15 - 4 = 11

p = 1, d = 1, q = 3,
P = 0, D = 1, Q = 5, S = 12,

```{r}
Box.test(resii, lag=15, type = c("Box-Pierce"), fitdf = 4)
```
Box-Ljung test
```{r}
Box.test(resii, lag=15, type = c("Ljung-Box"), fitdf = 4)
```
McLeod-Li test
```{r}
Box.test(resii, lag=15, type = c("Ljung-Box"), fitdf = 0)
```
We have all the p-values larger than .05, so model ii passes the diagnostic tests

```{r}
ar(resii, aic = TRUE, order.max = NULL, method = c("yule-walker"))
```
order selected is 0 and residuals resemble an AR(0) process, which is white noise.

Since the model has passed the diagnostic checkings, it is the final model.
SARIMA (p=11,d=1,q=3)x(P=0,D=1,Q=5)$_{s=12}$

### Forcast
```{r}
test_log <- log(test_ts)
```

```{r}
sarima.for(train_log, n.ahead=24 ,p=1, d=1, q=3, P=0, D=1, Q=5, S=12)
lines(test_log, col="blue", type="b")
```